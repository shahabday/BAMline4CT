{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahabday/BAMline4CT/blob/main/DSR_41_HuggingFace_GPT2_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENuTHenyD91-"
      },
      "source": [
        "# Text Generation with Hugging Face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0J0FqEyEDPa"
      },
      "source": [
        "# Part 1: Dealing with the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## Check if a GPU is available.\n",
        "\n",
        "If not, activate it in Runtime -> Change Runtime Type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgnF0xmTaVJ_"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers[torch] datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-F3M2WNEWPp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "import tqdm\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "tf.config.list_physical_devices(\"GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-2RJg-BE-LO"
      },
      "source": [
        "## Load the dataset. First from the internet. Then from the hard drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPywz4N5BYWW"
      },
      "outputs": [],
      "source": [
        "dataset_file = \"dataset.txt\"\n",
        "\n",
        "# How many files to load.\n",
        "file_number = 100\n",
        "\n",
        "# Clone the repo.\n",
        "!git clone https://github.com/vilmibm/lovecraftcorpus\n",
        "\n",
        "# Find all the files.\n",
        "paths = glob.glob(\"lovecraftcorpus/*.txt\")\n",
        "\n",
        "# Do not use all.\n",
        "paths = paths[:file_number]\n",
        "print(sorted(paths))\n",
        "\n",
        "# Merge.\n",
        "with open(dataset_file, \"w\") as output_file:\n",
        "    for path in paths:\n",
        "        for line in open(path, \"r\"):\n",
        "            for split in line.split(\"\\n\"):\n",
        "                split = split.strip()\n",
        "                if split != \"\":\n",
        "                    print(split, file=output_file)\n",
        "\n",
        "# Delete repo.\n",
        "!rm -rf lovecraftcorpus\n",
        "\n",
        "# Done.\n",
        "print(\"Corpus downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qNLKWdLEuEj"
      },
      "outputs": [],
      "source": [
        "raw_datasets = load_dataset(\"text\", data_files=[dataset_file])\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdsAkrgaFGqf"
      },
      "source": [
        "Let us look at an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oizu6moyFA_A"
      },
      "outputs": [],
      "source": [
        "for index in range(10):\n",
        "    token_sequence = raw_datasets[\"train\"][index][\"text\"]\n",
        "    print(token_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnBUMINdGccB"
      },
      "source": [
        "# Part 2: Training GPT-2.\n",
        "\n",
        "---\n",
        "\n",
        "## Train the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NTb_LYYDkU4"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer = BpeTrainer(vocab_size=5000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "def batch_iterator(batch_size=1000):\n",
        "    for i in range(0, len(raw_datasets[\"train\"]), batch_size):\n",
        "        yield raw_datasets[\"train\"][i : i + batch_size][\"text\"]\n",
        "\n",
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(raw_datasets[\"train\"]))\n",
        "tokenizer.save(\"tokenizer.json\")\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oQFOog5EYTV"
      },
      "outputs": [],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFyFYaWaG1bc"
      },
      "outputs": [],
      "source": [
        "tokenizer.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEOlHRm0ZlWA"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCZkCq0iGxg7"
      },
      "source": [
        "## Tokenize some samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7i6MwOFG8-b"
      },
      "source": [
        "Inspect the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA4BhB0qGtie"
      },
      "outputs": [],
      "source": [
        "token_sequence = raw_datasets[\"train\"][3][\"text\"]\n",
        "print(token_sequence)\n",
        "\n",
        "indices = tokenizer(token_sequence)[\"input_ids\"]\n",
        "print(indices)\n",
        "\n",
        "tokens = [tokenizer.decode([index]) for index in indices]\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCp0mFSW-xh3"
      },
      "outputs": [],
      "source": [
        "lengths = []\n",
        "for token_sequence in tqdm.tqdm(raw_datasets[\"train\"]):\n",
        "    token_sequence = token_sequence[\"text\"]\n",
        "    indices = tokenizer(token_sequence)[\"input_ids\"]\n",
        "    lengths += [len(indices)]\n",
        "\n",
        "plt.hist(lengths, bins=50)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o9UvSpTHHER"
      },
      "source": [
        "## Train the model.\n",
        "\n",
        "Prepare the tokenization function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dNy9ThsHAke"
      },
      "outputs": [],
      "source": [
        "sequence_length = 256\n",
        "\n",
        "def tokenize_function(example):\n",
        "    tokenized_example = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=sequence_length,\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": tokenized_example[\"input_ids\"]\n",
        "    }\n",
        "\n",
        "# Check a sample.\n",
        "token_sequence = raw_datasets[\"train\"][0]\n",
        "print(token_sequence)\n",
        "tokenized = tokenize_function(token_sequence)\n",
        "assert list(tokenized.keys()) == [\"input_ids\"], list(tokenized.keys())\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H34I1OLHSSg"
      },
      "source": [
        "Create the tokenized dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2cZX-S3HRr9"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
        "\n",
        "# Check a sample.\n",
        "tokenized = tokenized_datasets[\"train\"][0]\n",
        "assert list(tokenized.keys()) == [\"input_ids\"], list(tokenized.keys())\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF09ENCcJQJs"
      },
      "source": [
        "Instantiate a data collator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HItumD-wHNCU"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXG4-rfqJRh4"
      },
      "source": [
        "Create the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9wyjd8cJMBN"
      },
      "outputs": [],
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW34XvSQJare"
      },
      "source": [
        "## Test the data collator and the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dLuSRbIJXHi"
      },
      "outputs": [],
      "source": [
        "inputs = [tokenized_datasets[\"train\"][2]]\n",
        "inputs = data_collator(inputs)\n",
        "assert list(inputs.keys()) == [\"input_ids\", \"attention_mask\", \"labels\"], list(inputs.keys())\n",
        "print(\"input_ids:\", inputs[\"input_ids\"])\n",
        "print(\"\")\n",
        "\n",
        "outputs = model(**inputs)\n",
        "assert list(outputs.keys()) == [\"loss\", \"logits\", \"past_key_values\"], list(outputs.keys())\n",
        "print(\"logits:\", outputs[\"logits\"])\n",
        "\n",
        "plt.plot(outputs[\"logits\"].detach().numpy()[0][0])\n",
        "plt.title(\"Logits\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "activations = torch.nn.functional.softmax(outputs[\"logits\"], dim=-1)\n",
        "plt.plot(activations.detach().numpy()[0][0])\n",
        "plt.title(\"Activations\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBysGfUcJpAO"
      },
      "source": [
        "---\n",
        "\n",
        "## Run the training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIT_jnzIJcr7"
      },
      "outputs": [],
      "source": [
        "# Get the output directory with timestamp.\n",
        "output_path = \"output\"\n",
        "\n",
        "# Create the trainer.\n",
        "print(\"Creating trainer...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_path,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=72,\n",
        "    prediction_loss_only=False,\n",
        "    #report_to=\"none\"\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        ")\n",
        "\n",
        "# Train the model.\n",
        "trainer.train()\n",
        "\n",
        "# Save the tokenizer.\n",
        "tokenizer.save_pretrained(output_path)\n",
        "\n",
        "# Save the model.\n",
        "model.save_pretrained(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRTewg3jaX37"
      },
      "outputs": [],
      "source": [
        "inputs = [tokenized_datasets[\"train\"][2]]\n",
        "inputs = data_collator(inputs)\n",
        "assert list(inputs.keys()) == [\"input_ids\", \"attention_mask\", \"labels\"], list(inputs.keys())\n",
        "print(\"input_ids:\", inputs[\"input_ids\"])\n",
        "print(\"\")\n",
        "\n",
        "outputs = model(**inputs.to(\"cuda:0\"))\n",
        "assert list(outputs.keys()) == [\"loss\", \"logits\", \"past_key_values\"], list(outputs.keys())\n",
        "print(\"logits:\", outputs[\"logits\"])\n",
        "\n",
        "plt.plot(outputs[\"logits\"].cpu().detach().numpy()[0][0])\n",
        "plt.title(\"Logits\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "activations = torch.nn.functional.softmax(outputs[\"logits\"], dim=-1)\n",
        "plt.plot(activations.cpu().detach().numpy()[0][0])\n",
        "plt.title(\"Activations\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAf31d_WKw8L"
      },
      "source": [
        "## How to generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAUsQ202Jr2i"
      },
      "outputs": [],
      "source": [
        "model.to(\"cuda\")\n",
        "\n",
        "# Encode the conditioning tokens.\n",
        "input_ids = tokenizer.encode(\"The most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents.\", return_tensors=\"pt\").cuda()\n",
        "print(input_ids)\n",
        "\n",
        "# Generate more tokens.\n",
        "generated_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=100,\n",
        "    do_sample=True,\n",
        "    temperature=0.5\n",
        ")\n",
        "generated_sequence = tokenizer.decode(generated_ids[0], clean_up_tokenization_spaces=True)\n",
        "print(generated_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTFXN67wLN2u"
      },
      "source": [
        "#Thank you!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}